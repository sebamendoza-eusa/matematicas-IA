Vectores y escalares: la base del lenguaje de la IA
===

Definiciones
---

Los conceptos de **vectores** y **escalares**, aunque parezcan abstractos, son la base sobre la que se construyen los modelos de *machine learning* y *deep learning*.

Para empezar, imaginemos que **los escalares son los "nÃºmeros"** con los que estamos familiarizados, es decir, una magnitud Ãºnica. Un escalar representa una sola cantidad. Por ejemplo, la temperatura de una habitaciÃ³n (25Â°C), el precio de un producto (50â‚¬) o la edad de una persona (30 aÃ±os) son todos ejemplos de escalares. En el contexto de la programaciÃ³n, podrÃ­amos pensarlo simplemente como una variable que almacena un valor numÃ©rico. En un modelo de IA, un escalar podrÃ­a ser un Ãºnico *hiperparÃ¡metro*, como la tasa de aprendizaje.

Por otro lado, un **vector** es una colecciÃ³n ordenada de escalares. Es mÃ¡s que una simple lista de nÃºmeros; es una entidad matemÃ¡tica que tiene tanto magnitud como direcciÃ³n. Visualmente, podemos representar un vector como una flecha en el espacio, que parte de un punto de origen y apunta hacia otro. Cada uno de los escalares que lo componen se denomina "componente" del vector.

> **Ejemplo:**
> 
> En un sistema de recomendaciÃ³n de pelÃ­culas, el perfil de un usuario podrÃ­a representarse como un vector. Cada componente del vector podrÃ­a indicar la calificaciÃ³n que el usuario le ha dado a una pelÃ­cula especÃ­fica. Por ejemplo, un vector como [5,3,4,1] podrÃ­a significar que el usuario calificÃ³ la pelÃ­cula 1 con 5 estrellas, la pelÃ­cula 2 con 3, etc. Cada calificaciÃ³n es un escalar, y juntos forman el vector que representa el perfil de preferencias del usuario.

En el mundo de la IA, los vectores son omnipresentes. Los datos con los que trabajamos, como imÃ¡genes, texto o audio, se transforman en representaciones numÃ©ricas que son, en esencia, vectores. Por ejemplo, una imagen digital puede ser un vector donde cada componente es la intensidad de color de un pÃ­xel. Un texto puede ser un vector donde cada componente representa la frecuencia de una palabra en un vocabulario.

La capacidad de representar informaciÃ³n compleja en forma de vectores es lo que permite que los algoritmos de aprendizaje automÃ¡tico puedan procesar y encontrar patrones en los datos. El Ã¡lgebra lineal nos proporciona las herramientas para manipular estos vectores, permitiÃ©ndonos realizar operaciones como la suma, la resta, o el producto escalar, que son fundamentales para los cÃ¡lculos internos de los modelos.

**Para reflexionar...**

> **Â¿Por quÃ© es mÃ¡s Ãºtil representar los datos como vectores en lugar de simplemente como una lista de nÃºmeros sin ninguna estructura?**
> 
> Considera cÃ³mo la estructura de un vector, y la capacidad de realizar operaciones matemÃ¡ticas con Ã©l, nos permite capturar no solo la informaciÃ³n individual, sino tambiÃ©n las relaciones y distancias entre los distintos puntos de datos. Piensa en cÃ³mo calcular la "distancia" entre dos perfiles de usuario, o la "similitud" entre dos imÃ¡genes, se convierte en un problema matemÃ¡tico manejable cuando los datos estÃ¡n representados como vectores.

## Ejemplo prÃ¡ctico: Vectores y escalares en Machine Learning

Para entender mejor cÃ³mo estos conceptos se aplican en la prÃ¡ctica, pensemos en un problema comÃºn en el campo del *machine learning*: la clasificaciÃ³n. Supongamos que estamos construyendo un modelo para predecir si un correo electrÃ³nico es spam o no.

Un **escalar** podrÃ­a ser un valor Ãºnico asociado a un correo. Por ejemplo, la longitud del correo en caracteres, el nÃºmero de enlaces que contiene, o la cantidad de veces que aparece una palabra especÃ­fica, como "gratis". Cada uno de estos valores es una sola magnitud, un escalar que describe una caracterÃ­stica del correo electrÃ³nico.

Sin embargo, para tomar una decisiÃ³n informada, el modelo necesita considerar mÃºltiples caracterÃ­sticas a la vez. AquÃ­ es donde entran en juego los **vectores**. Podemos representar cada correo electrÃ³nico como un vector **de caracterÃ­sticas**. Cada componente de este vector es un escalar que representa una de las caracterÃ­sticas que hemos medido. Por ejemplo, un vector para un correo podrÃ­a ser

$$
\text{correo-ejemplo=[longitud,enlaces,frecuencia-gratis,â€¦]}
$$

Si nuestro modelo considera la longitud del correo, el nÃºmero de enlaces y la frecuencia de la palabra "gratis", un correo concreto podrÃ­a ser representado por el vector [500,3,5].

- 500 es el escalar que representa la longitud.
- 3 es el escalar que representa el nÃºmero de enlaces.
- 5 es el escalar que representa la frecuencia de la palabra "gratis".

Este vector, con tres dimensiones, nos da una "huella" numÃ©rica del correo electrÃ³nico. A medida que recopilamos mÃ¡s datos, estos vectores se convierten en los puntos con los que el algoritmo de aprendizaje trabaja. El modelo busca patrones en estos puntos: por ejemplo, podrÃ­a aprender que los correos que son spam tienden a tener vectores con valores altos en las componentes de "enlaces" y "frecuencia de 'gratis'".

Las **operaciones con vectores** nos permiten medir la "distancia" o la "similitud" entre correos electrÃ³nicos, lo cual es fundamental para muchos algoritmos de clasificaciÃ³n. El producto escalar, por ejemplo, es una operaciÃ³n que nos permite determinar cuÃ¡n similares son dos vectores en direcciÃ³n, lo cual se traduce en quÃ© tan similares son dos correos en sus caracterÃ­sticas. De esta manera, las matemÃ¡ticas abstractas se convierten en las herramientas prÃ¡cticas que permiten a la IA tomar decisiones.

**Para reflexionar...**

> Â¿CÃ³mo crees que podrÃ­amos representar una imagen en blanco y negro como un vector para que un algoritmo de IA la procese? Â¿Y una imagen a color?
> 
> Considera cÃ³mo los pÃ­xeles de una imagen pueden ser vistos como los componentes de un vector. Piensa en quÃ© representa el valor de cada pÃ­xel y cÃ³mo el color podrÃ­a aÃ±adir mÃ¡s "dimensiones" a este vector.

Operaciones bÃ¡sicas con vectores
---

Ahora que hemos comprendido el concepto de vector como una representaciÃ³n de datos, es crucial entender cÃ³mo podemos manipularlos a travÃ©s de operaciones matemÃ¡ticas. Estas operaciones estÃ¡n en la base de la gran mayorÃ­a de algoritmos usados en IA.

### Suma y Resta de vectores

La suma y resta de vectores se realiza componente a componente. Para sumar dos vectores, simplemente se suman sus elementos correspondientes. Lo mismo aplica para la resta. Para vectores $\mathbf{v} = [v_1, v_2, \dots, v_n]$ y $\mathbf{w} = [w_1, w_2, \dots, w_n]$, la suma es:

$$
\mathbf{v} + \mathbf{w} = [v_1 + w_1, v_2 + w_2, \dots, v_n + w_n]
$$

En el contexto de la IA, estas operaciones pueden ser Ãºtiles para combinar o ajustar representaciones de datos. Por ejemplo, en el **procesamiento del lenguaje natural (NLP)**, la suma de vectores de palabras (**word embeddings**) puede capturar relaciones semÃ¡nticas. El vector de la palabra "Rey" mÃ¡s el vector de "Mujer" podrÃ­a ser similar al vector de "Reina".

> **Ejemplo**:
> En un sistema de recomendaciÃ³n, si un usuario estÃ¡ representado por un vector de preferencias $\mathbf{u}$ y la pelÃ­cula que ve estÃ¡ representada por un vector de caracterÃ­sticas $\mathbf{p}$, un ajuste en la preferencia del usuario despuÃ©s de ver la pelÃ­cula podrÃ­a modelarse como $\mathbf{u}_{\text{nueva}} = \mathbf{u} + \mathbf{p}_{\text{ajuste}}$, donde $\mathbf{p}_{\text{ajuste}}$ es un pequeÃ±o vector que modifica las preferencias del usuario.

### NormalizaciÃ³n de vectores

La normalizaciÃ³n de un vector es el proceso de ajustar su longitud a una unidad, sin cambiar su direcciÃ³n. Esto se logra dividiendo cada componente del vector por su **norma** (su longitud). La norma, o magnitud, de un vector $\mathbf{v}$ se calcula como la raÃ­z cuadrada de la suma de los cuadrados de sus componentes.

$$
\Vert\mathbf{v}\Vert = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2}
$$

El vector normalizado $\hat{\mathbf{v}}$ es:

$$
\hat{\mathbf{v}} = \frac{\mathbf{v}}{\Vert\mathbf{v}\Vert}
$$

La normalizaciÃ³n es esencial en muchos algoritmos de IA, especialmente en aquellos que utilizan distancias o productos escalares. Al normalizar los vectores, se asegura que la magnitud de los datos no influya indebidamente en el cÃ¡lculo, haciendo que el modelo se enfoque en la direcciÃ³n y la relaciÃ³n entre ellos. Esto es crucial para **regular la magnitud de los gradientes** en el *deep learning*, evitando problemas como el **vanishing gradient**.

**Para reflexionar...**

> **Â¿CÃ³mo podrÃ­a el producto escalar ser utilizado para determinar la "similitud" entre dos documentos de texto? Â¿QuÃ© desafÃ­os crees que podrÃ­an surgir si los vectores de palabras no estuvieran normalizados?**
> *Considera que los documentos de texto pueden ser representados como vectores donde cada componente es la frecuencia de una palabra. Piensa en cÃ³mo el producto escalar mide la similitud direccional y cÃ³mo la normalizaciÃ³n asegura que un documento mÃ¡s largo no sea automÃ¡ticamente mÃ¡s "similar" a otro.*

### Producto escalar

El producto escalar de dos vectores es una de las operaciones mÃ¡s importantes en la IA. A diferencia de la suma, el resultado de esta operaciÃ³n no es otro vector, sino un Ãºnico escalar. Se calcula multiplicando los elementos correspondientes de los dos vectores y sumando todos los productos.

$$
\mathbf{v} \cdot \mathbf{w} = \sum_{i=1}^n v_i w_i = v_1w_1 + v_2w_2 + \dots + v_nw_n
$$

Podemos imaginar el valor del producto escalar como una medida de la relaciÃ³n geomÃ©trica entre los dos vectores. Un producto escalar grande y positivo indica que los vectores apuntan en direcciones similares. Si es cero, son ortogonales (perpendiculares). Si es grande y negativo, apuntan en direcciones opuestas.

Esta propiedad es fundamental en los algoritmos de **regresiÃ³n lineal** y en las **redes neuronales**. En una neurona, el producto escalar entre el vector de entradas y el vector de pesos determina la activaciÃ³n de la neurona, es decir, el grado en que las entradas influyen en la salida. TambiÃ©n es una operaciÃ³n crucial en el procesamiento del lenguaje natural.

### InterpretaciÃ³n geomÃ©trica del producto escalar

Para ser mÃ¡s rigurosos, profundicemos en la interpretaciÃ³n geomÃ©trica del **producto escalar**. MÃ¡s allÃ¡ de ser una simple operaciÃ³n algebraica, el producto escalar de dos vectores, $\mathbf{v}$ y $\mathbf{w}$, estÃ¡ Ã­ntimamente relacionado con la longitud de estos vectores y el Ã¡ngulo que se forma entre ellos.

La fÃ³rmula que nos permite entender esta relaciÃ³n es:

$$
\mathbf{v} \cdot \mathbf{w} = \Vert\mathbf{v}\Vert \Vert\mathbf{w}\Vert \cos(\theta)
$$

Donde $\Vert\mathbf{v}\Vert$ y $\Vert\mathbf{w}\Vert$ son las normas (o longitudes) de los vectores $\mathbf{v}$ y $\mathbf{w}$, respectivamente, y $\cos(\theta)$ es el coseno del Ã¡ngulo $\theta$ que forman los dos vectores.

Esta fÃ³rmula revela la verdadera naturaleza del producto escalar. Nos dice que el producto escalar es una medida de la **proyecciÃ³n de un vector sobre otro**. Piensa en la proyecciÃ³n como la "sombra" que un vector arroja sobre el otro. El producto escalar es el producto de la longitud de esa sombra por la longitud del vector sobre el que se proyecta.

- Cuando los vectores apuntan en la misma direcciÃ³n ($\theta = 0^\circ$), el coseno es $1$. El producto escalar es positivo y mÃ¡ximo, igual al producto de sus longitudes.
- Cuando los vectores son **ortogonales** (perpendiculares, $\theta = 90^\circ$), el coseno es $0$. El producto escalar es cero, lo que significa que no hay proyecciÃ³n de un vector sobre el otro.
- Cuando los vectores apuntan en direcciones opuestas ($\theta = 180^\circ$), el coseno es $-1$. El producto escalar es negativo y su valor absoluto es mÃ¡ximo.

Esta interpretaciÃ³n es la clave para entender por quÃ© el producto escalar se usa para medir la **similitud de cosenos** en la IA. Al normalizar los vectores (dividiendo por sus normas), la fÃ³rmula se simplifica a $\cos(\theta) = \frac{\mathbf{v} \cdot \mathbf{w}}{\Vert\mathbf{v}\Vert \Vert\mathbf{w}\Vert}$. El valor resultante es el coseno del Ã¡ngulo entre los vectores. Cuanto mÃ¡s cercano a 1 sea este valor, mÃ¡s similares son los vectores en direcciÃ³n, lo que se traduce en una mayor similitud entre los datos que representan.

Esta tÃ©cnica se utiliza, por ejemplo, en los sistemas de recomendaciÃ³n para encontrar elementos similares a los que un usuario ya ha valorado positivamente, o en el procesamiento del lenguaje natural para medir la similitud semÃ¡ntica entre palabras o documentos.

---

**Para reflexionar...**

> **Â¿QuÃ© diferencia fundamental existe entre la similitud de coseno y la distancia euclidiana (la longitud de la lÃ­nea recta que une los puntos) para medir la similitud entre dos vectores? Â¿En quÃ© escenarios de la IA crees que una podrÃ­a ser mÃ¡s adecuada que la otra?**
> *Considera que la similitud de coseno se centra en la direcciÃ³n de los vectores, mientras que la distancia euclidiana considera tanto la direcciÃ³n como la magnitud. Piensa en el problema de la clasificaciÃ³n de documentos de texto, donde la longitud del documento puede variar enormemente, o en el caso de la detecciÃ³n de anomalÃ­as, donde la magnitud de los datos puede ser un factor clave.*

> **Ejemplo**:
> Imaginemos que queremos predecir el precio de una casa. El vector de caracterÃ­sticas de la casa podrÃ­a ser $\mathbf{c} = [\text{metros}^2, \text{habitaciones}, \text{baÃ±os}]$ y el modelo de regresiÃ³n lineal tiene un vector de pesos $\mathbf{w} = [w_1, w_2, w_3]$. La predicciÃ³n del precio $\hat{y}$ se calcula como el producto escalar de ambos vectores, mÃ¡s un tÃ©rmino de sesgo (bias): $\hat{y} = \mathbf{w} \cdot \mathbf{c} + b$.

Espacios Vectoriales: la estructura del universo de datos de la IA
---

Para que la **Inteligencia Artificial** pueda procesar y aprender de la informaciÃ³n, los datos deben estar estructurados en un marco matemÃ¡tico coherente. Este marco constituye un concepto fundamental conocido como **espacio vectorial**. Formalmente, un espacio vectorial es un conjunto de elementos, a los que llamamos **vectores**, en el que se definen dos operaciones: la suma de vectores y la multiplicaciÃ³n de un vector por un escalar. En el Ã¡mbito del *machine learning*, nos referimos a este espacio como el **espacio de caracterÃ­sticas**, donde cada instancia de datos (como un documento de texto o una imagen) es un vector cuya posiciÃ³n estÃ¡ determinada por los valores de sus caracterÃ­sticas.

> Ejemplo:
> 
> Imagina un modelo que clasifica el sentimiento de reseÃ±as de pelÃ­culas. El espacio de caracterÃ­sticas podrÃ­a tener una dimensiÃ³n para la frecuencia de la palabra "genial", otra para "aburrido", y otra para "increÃ­ble". Una reseÃ±a, como "La pelÃ­cula fue genial y muy increÃ­ble", serÃ­a representada como un vector como [1,0,1]. Este vector es un punto en el espacio de caracterÃ­sticas que representa la reseÃ±a.

### Espacio vectorial: una definiciÃ³n formal

Un **espacio vectorial** es una estructura matemÃ¡tica fundamental que organiza el trabajo con vectores de manera coherente. No basta con entender un vector como â€œuna lista de nÃºmerosâ€: necesitamos un marco que establezca reglas claras sobre cÃ³mo interactÃºan esos vectores entre sÃ­ y con los escalares que los multiplican.

Formalmente, un espacio vectorial es un conjunto $V$ cuyos elementos son vectores, definido sobre un **cuerpo** (o campo) $\mathbb{K}$ â€”generalmente los nÃºmeros reales $\mathbb{R}$ o los complejos $\mathbb{C}$â€”, junto con dos operaciones:

1. **Suma de vectores:** una aplicaciÃ³n $+: V \times V \to V$ que combina dos vectores de $V$ y devuelve otro vector de $V$.
2. **MultiplicaciÃ³n por escalares:** una aplicaciÃ³n $\cdot : \mathbb{K} \times V \to V$ que toma un escalar $a \in \mathbb{K}$ y un vector $\mathbf{v} \in V$, y produce otro vector $a\mathbf{v} \in V$.

Para que $(V, +, \cdot)$ sea un espacio vectorial, estas operaciones deben cumplir **ocho axiomas fundamentales**:

* **Clausura bajo la suma y la multiplicaciÃ³n escalar:** la suma de dos vectores y el producto de un vector por un escalar permanecen en $V$. Es decir, son **operaciones de composiciÃ³n interna**
* **Asociatividad y conmutatividad de la suma:** $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$, y $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$.
* **Existencia del vector neutro:** existe un vector $\mathbf{0} \in V$ tal que $\mathbf{v} + \mathbf{0} = \mathbf{v}$ para todo $\mathbf{v}$.
* **Existencia del opuesto:** para cada $\mathbf{v} \in V$ existe un vector $-\mathbf{v}$ tal que $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$.
* **Compatibilidad de la multiplicaciÃ³n escalar con el producto de escalares:** $(ab)\mathbf{v} = a(b\mathbf{v})$.
* **Elemento neutro para el producto escalar:** $1 \cdot \mathbf{v} = \mathbf{v}$ para todo $\mathbf{v} \in V$.
* **Distributividad de la multiplicaciÃ³n escalar respecto a la suma de escalares:** $(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$.
* **Distributividad de la multiplicaciÃ³n escalar respecto a la suma de vectores:** $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$.

Con estas reglas, el espacio vectorial se convierte en un entorno robusto donde cualquier combinaciÃ³n lineal de vectores tiene sentido y se mantiene dentro del conjunto $V$.

> **Ejemplo:**
> El conjunto $\mathbb{R}^n$ de todas las $n$-tuplas de nÃºmeros reales, con la suma y multiplicaciÃ³n definidas componente a componente, es el espacio vectorial mÃ¡s habitual en *machine learning*. Cada vector representa un dato con $n$ caracterÃ­sticas, y $\mathbb{R}$ es el cuerpo de escalares que lo acompaÃ±a.

### Dependencia lineal, base y cambio de base

Dentro de este espacio, la nociÃ³n de **dependencia lineal** es de suma importancia. Un conjunto de vectores es linealmente dependiente si al menos uno de ellos puede ser expresado como una **combinaciÃ³n lineal** de los demÃ¡s. Una combinaciÃ³n lineal no es mÃ¡s que una suma de vectores multiplicados por escalares. Si podemos construir un vector a partir de otros, esto nos indica que ese vector no contiene informaciÃ³n nueva y, por lo tanto, es redundante. . Esta redundancia es un problema serio para los algoritmos, a menudo manifestado como **multicolinealidad**, que confunde a los modelos y puede llevar a soluciones inestables.

> Ejemplo:
> 
> Considera un modelo que usa dos caracterÃ­sticas: la edad de un cliente en aÃ±os y su edad en dÃ­as. El vector que representa "edad en dÃ­as" es simplemente el vector "edad en aÃ±os" multiplicado por el escalar 365. Estos dos vectores son linealmente dependientes, ya que uno es una combinaciÃ³n lineal del otro. Un modelo de regresiÃ³n lineal no podrÃ­a determinar la contribuciÃ³n Ãºnica de cada caracterÃ­stica al resultado final.

La comprensiÃ³n de la independencia lineal nos lleva a un concepto crucial: la **base** de un espacio vectorial. Una base es el conjunto mÃ¡s pequeÃ±o de vectores linealmente independientes que nos permite representar cualquier otro vector en el espacio a travÃ©s de una combinaciÃ³n lineal. Piensa en la base como el sistema de coordenadas subyacente de nuestro universo de datos.

> Ejemplo:
> 
> En el caso de los datos de reseÃ±as de pelÃ­culas, si solo usamos las palabras "genial" y "aburrido", nuestra base estarÃ­a formada por el vector de "genial" [1,0] y el vector de "aburrido" [0,1]. Cualquier otra reseÃ±a en este espacio puede ser descrita como una combinaciÃ³n de estos dos vectores.

La habilidad de encontrar una nueva base que sea mÃ¡s eficiente para nuestro problema es la clave de las tÃ©cnicas de **reducciÃ³n de dimensionalidad**, como el **AnÃ¡lisis de Componentes Principales (PCA)**. Estos algoritmos buscan una nueva base que capture la mayor varianza de los datos con el menor nÃºmero de dimensiones. Al proyectar nuestros datos en esta nueva base, eliminamos la redundancia y comprimimos la informaciÃ³n, lo que no solo mejora la eficiencia computacional de los modelos, sino que tambiÃ©n puede reducir el ruido y mejorar su rendimiento.

---

**Para reflexionar...**

> Si un modelo de machine learning para predecir precios de casas utiliza como caracterÃ­sticas tanto el tamaÃ±o de la casa en metros cuadrados como el tamaÃ±o en pies cuadrados, Â¿quÃ© implicaciones tendrÃ­a esto en la dependencia lineal de los datos? Â¿Por quÃ© los algoritmos de reducciÃ³n de dimensionalidad, como el AnÃ¡lisis de Componentes Principales (PCA), buscan encontrar una nueva base para los datos?
> 
> Considera cÃ³mo el tamaÃ±o en metros y pies cuadrados son redundantemente lineales. Piensa en cÃ³mo una nueva base podrÃ­a permitirnos capturar la misma informaciÃ³n en un espacio de menor dimensiÃ³n, eliminando esta redundancia y facilitando el trabajo del modelo.

#### Cambio de base en espacios vectoriales

Si en un espacio vectorial, la **base** es el conjunto mÃ­nimo de vectores linealmente independientes que permite representar cualquier otro vector mediante combinaciones lineales. Cambiar de base significa expresar los mismos vectores en un sistema de coordenadas diferente. Aunque los vectores en sÃ­ no cambian â€”siguen ocupando el mismo lugar en el espacioâ€”, sus **coordenadas** sÃ­ lo hacen, porque ahora se describen con respecto a nuevos ejes de referencia.

MatemÃ¡ticamente, si tenemos una base original $B = {\mathbf{e}_1, \mathbf{e}_2}$ y una nueva base $B' = {\mathbf{b}_1, \mathbf{b}_2}$, cualquier vector $\mathbf{v}$ que en la base original se expresa como

$$
\mathbf{v} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2
$$

puede representarse en la nueva base como

$$
\mathbf{v} = y_1 \mathbf{b}_1 + y_2 \mathbf{b}_2
$$

donde $(x_1, x_2)$ y $(y_1, y_2)$ son las coordenadas del mismo vector en bases distintas.

---

##### Ejemplo en dos dimensiones

Consideremos el plano $\mathbb{R}^2$. La base canÃ³nica estÃ¡ formada por los vectores

$$
\mathbf{e}_1 = \begin{bmatrix}1 \\ 0\end{bmatrix}, \quad 
\mathbf{e}_2 = \begin{bmatrix}0 \\ 1\end{bmatrix}.
$$

Supongamos que definimos una nueva base $B'$ con los vectores

$$
\mathbf{b}_1 = \begin{bmatrix}1 \\ 1\end{bmatrix}, \quad 
\mathbf{b}_2 = \begin{bmatrix}-1 \\ 1\end{bmatrix}.
$$

Ahora tomemos un vector en coordenadas canÃ³nicas:

$$
\mathbf{v} = \begin{bmatrix}2 \\ 3\end{bmatrix}.
$$

Para expresarlo en la nueva base $B'$, necesitamos encontrar los escalares $y_1$ y $y_2$ que cumplen:

$$
\mathbf{v} = y_1 \mathbf{b}_1 + y_2 \mathbf{b}_2
= y_1 \begin{bmatrix}1 \\ 1\end{bmatrix} + y_2 \begin{bmatrix}-1 \\ 1\end{bmatrix}.
$$

Es decir:

$$
\begin{bmatrix}2 \\ 3\end{bmatrix} = \begin{bmatrix} y_1 - y_2 \\ y_1 + y_2 \end{bmatrix}.
$$

Esto nos da el sistema:

$$
\begin{cases}
y_1 - y_2 = 2 \\
y_1 + y_2 = 3
\end{cases}
$$

Resolviendo:

$$
y_1 = \tfrac{5}{2}, \quad y_2 = \tfrac{1}{2}.
$$

Por tanto, en la base $B'$ el vector $\mathbf{v}$ se representa como

$$
[\mathbf{v}]_{B'} = \begin{bmatrix}\tfrac{5}{2} \\ \tfrac{1}{2}\end{bmatrix}.
$$

El vector $\mathbf{v}$ sigue ocupando la misma posiciÃ³n en el plano, pero dependiendo de la base elegida, sus **coordenadas numÃ©ricas cambian**. En la base canÃ³nica lo describimos como $(2,3)$, mientras que en la base $B'$ lo describimos como $(2.5, 0.5)$.

Este cambio de perspectiva es esencial en IA porque permite elegir bases que simplifiquen los cÃ¡lculos o eliminen redundancias en los datos, como ocurre en mÃ©todos de reducciÃ³n de dimensionalidad (por ejemplo, PCA).

---

> **Para reflexionarâ€¦**
> Â¿QuÃ© ventajas podrÃ­a tener usar una base â€œadaptada a los datosâ€ en lugar de la base canÃ³nica? Â¿Por quÃ© crees que tÃ©cnicas como PCA buscan una nueva base que maximice la varianza explicada de los datos?

Perfecto ğŸ™Œ. Te preparo un **apartado en formato apuntes** sobre las aplicaciones del cambio de base en IA, con el mismo estilo narrativo y didÃ¡ctico que venimos usando.

------

#### Aplicaciones del cambio de base en inteligencia artificial

El cambio de base no es un simple artificio matemÃ¡tico, sino una herramienta poderosa en el tratamiento de datos para la inteligencia artificial. Al modificar el sistema de referencia en el que representamos los vectores, los datos conservan su esencia, pero adquieren nuevas propiedades que pueden ser mÃ¡s Ãºtiles para los algoritmos de aprendizaje.

Una primera aplicaciÃ³n es la **decorrelaciÃ³n de variables**. En muchos problemas de aprendizaje supervisado, las caracterÃ­sticas originales estÃ¡n fuertemente correlacionadas entre sÃ­, lo que provoca inestabilidad en modelos como la regresiÃ³n lineal o la logÃ­stica. Cambiar de base permite generar un nuevo sistema de coordenadas en el que las variables se comportan de manera mÃ¡s independiente, facilitando la interpretaciÃ³n y mejorando la robustez del modelo.

Otra aplicaciÃ³n esencial es la relacionada con las **mÃ©tricas de distancia y similitud**. Algoritmos como $k$-means o $k$-NN dependen crÃ­ticamente de cÃ³mo medimos la proximidad entre instancias. En la base original, dos puntos pueden parecer cercanos o lejanos de manera engaÃ±osa debido a la escala o correlaciÃ³n entre las variables. Al redefinir la base, podemos situar los datos en un espacio donde las distancias sean mÃ¡s significativas y reflejen mejor la estructura subyacente de los datos.

El cambio de base tambiÃ©n desempeÃ±a un papel clave en la **optimizaciÃ³n numÃ©rica**. En algoritmos como el descenso de gradiente, la forma de las curvas de nivel de la funciÃ³n de coste depende de la base en la que nos situemos. Si los datos se expresan en un sistema con escalas equilibradas y sin redundancias, el descenso se vuelve mÃ¡s directo y eficiente, evitando zigzags innecesarios y acelerando la convergencia hacia el mÃ­nimo.

En el Ã¡mbito de la **extracciÃ³n de caracterÃ­sticas latentes**, el cambio de base permite descubrir factores ocultos que explican la variabilidad de los datos. TÃ©cnicas como el anÃ¡lisis de componentes principales (PCA), el anÃ¡lisis de componentes independientes (ICA) o los autoencoders se apoyan en esta idea: encontrar un nuevo sistema de referencia que concentre la informaciÃ³n en menos dimensiones o que revele estructuras que permanecÃ­an ocultas en la representaciÃ³n original.

Finalmente, el cambio de base puede favorecer la **interpretabilidad de los modelos**. En visiÃ³n por computador, por ejemplo, es habitual que tras una transformaciÃ³n los ejes del nuevo espacio se alineen con patrones visuales concretos, como bordes, texturas o colores. Estos nuevos ejes no solo simplifican el cÃ¡lculo, sino que tambiÃ©n facilitan la comprensiÃ³n del proceso interno por el que un modelo toma sus decisiones.

------

> **Para reflexionarâ€¦**
>  Si un modelo de clasificaciÃ³n no logra distinguir entre dos clases en el espacio de caracterÃ­sticas original, Â¿cÃ³mo podrÃ­a ayudar un cambio de base a que las clases fueran mÃ¡s separables? Â¿QuÃ© papel juega aquÃ­ la geometrÃ­a de los datos?

Perfecto ğŸ™Œ. Te propongo un **ejemplo prÃ¡ctico muy sencillo**, pensado para alumnos que aÃºn no han visto *machine learning*, pero que les permita **visualizar la utilidad del cambio de base** sin necesidad de algoritmos complejos.

---

##### Ejemplo prÃ¡ctico: cambio de base en un espacio de caracterÃ­sticas bidimensional

Imaginemos que tenemos un conjunto de puntos en el plano, cada uno representando a un estudiante segÃºn dos caracterÃ­sticas muy simples:

* **Horas de estudio** ($x$).
* **Horas de descanso** ($y$).

AsÃ­, un estudiante que estudia 4 horas y descansa 6 se representarÃ­a como el vector:

$$
\mathbf{v} = \begin{bmatrix}4 \\ 6\end{bmatrix}.
$$

En la **base canÃ³nica** (los ejes habituales $x$ e $y$), este vector se describe como â€œ4 hacia la derecha y 6 hacia arribaâ€.

Ahora pensemos que queremos analizar la relaciÃ³n entre el **equilibrio** entre estudio y descanso, y la **carga total** de horas. Para ello, definimos una **nueva base** con dos vectores:

$$
\mathbf{b}_1 = \begin{bmatrix}1 \\ 1\end{bmatrix}, \quad
\mathbf{b}_2 = \begin{bmatrix}1 \\ -1\end{bmatrix}.
$$

* $\mathbf{b}_1$ representa el eje â€œtotal de horasâ€ (suma de estudio y descanso).
* $\mathbf{b}_2$ representa el eje â€œdiferencia entre estudio y descansoâ€.

Queremos expresar el vector $\mathbf{v} = \begin{bmatrix}4 \ 6\end{bmatrix}$ en esta nueva base. Buscamos $y_1, y_2$ tales que:

$$
\mathbf{v} = y_1 \mathbf{b}_1 + y_2 \mathbf{b}_2
= y_1 \begin{bmatrix}1 \\ 1\end{bmatrix} + y_2 \begin{bmatrix}1 \\ -1\end{bmatrix}.
$$

Esto equivale a resolver:

$$
\begin{bmatrix}4 \\ 6\end{bmatrix} =
\begin{bmatrix} y_1 + y_2 \\ y_1 - y_2 \end{bmatrix}.
$$

De donde obtenemos el sistema:

$$
\begin{cases}
y_1 + y_2 = 4 \\
y_1 - y_2 = 6
\end{cases}
$$

Resolviendo:

$$
y_1 = 5, \quad y_2 = -1.
$$

En la nueva base, el mismo vector se representa como:

$$
[\mathbf{v}]_{B'} = \begin{bmatrix}5 \\ -1\end{bmatrix}.
$$

Es decir, en la base original, veÃ­amos al estudiante como â€œ4 horas de estudio, 6 horas de descansoâ€. En la nueva base, lo vemos como â€œ5 horas totales, con un desequilibrio de -1 (mÃ¡s descanso que estudio)â€.

El vector no ha cambiado de lugar en el plano, pero el **punto de vista matemÃ¡tico** nos ofrece ahora informaciÃ³n mÃ¡s alineada con la pregunta que queremos responder:

* Â¿CuÃ¡ntas horas totales dedica el estudiante?
* Â¿EstÃ¡ equilibrado entre estudio y descanso, o favorece uno sobre el otro?

Este ejemplo muestra cÃ³mo un cambio de base no altera los datos, pero **resalta propiedades distintas** segÃºn el problema que queramos analizar.

Elegir una base es como elegir el **punto de vista matemÃ¡tico** desde el que miramos los datos.

- Si el objetivo es eliminar redundancia, elegimos una base donde las variables estÃ©n decorrelacionadas (como en PCA).
- Si el objetivo es mejorar la separaciÃ³n entre clases, podemos buscar una base donde las categorÃ­as sean mÃ¡s distinguibles (como en *Linear Discriminant Analysis*).
- Si el objetivo es interpretar mejor, podemos escoger ejes que se correspondan con factores significativos (por ejemplo, â€œtotal de horasâ€ y â€œequilibrioâ€).

AsÃ­, el cambio de base es un paso consciente: no se trata de cambiar por cambiar, sino de **alinear la representaciÃ³n de los datos con el objetivo del anÃ¡lisis o del modelo**.

---

> **Para reflexionarâ€¦**
> Si en lugar de estudiantes quisiÃ©ramos analizar dietas (proteÃ­nas y carbohidratos), Â¿cÃ³mo podrÃ­amos definir una nueva base que refleje el â€œtotal de calorÃ­asâ€ y el â€œequilibrio entre nutrientesâ€?

