Matrices: Operaciones bÃ¡sicas y tipos especiales
===

DefiniciÃ³n
---

Continuamos nuestro recorrido por el **Ã¡lgebra lineal** orientada al uso en **Inteligencia Artificial**. Si los vectores nos permiten representar datos individuales, las **matrices** nos ofrecen la capacidad de estructurar y manipular grandes colecciones de datos de manera eficiente.

Una matriz no es mÃ¡s que una cuadrÃ­cula bidimensional de nÃºmeros. Piensa en ella como una tabla o una hoja de cÃ¡lculo. Cada fila y cada columna de esta matriz puede representar diferentes aspectos de nuestros datos. Por ejemplo, en un conjunto de datos, las filas podrÃ­an ser diferentes instancias de datos (como clientes o imÃ¡genes), y las columnas podrÃ­an ser las caracterÃ­sticas de cada instancia (edad, ingresos, o pÃ­xeles).

La forma de una matriz se describe por sus dimensiones, es decir, el nÃºmero de filas y columnas que posee. Una matriz con $m$ filas y $n$ columnas se denota como una matriz de $m \times n$.

> **Ejemplo**:
> En un conjunto de datos de un modelo de *machine learning*, podrÃ­amos tener una matriz donde cada fila es una casa y cada columna una caracterÃ­stica (nÃºmero de habitaciones, metros cuadrados, etc.). Si tuviÃ©ramos 100 casas y 3 caracterÃ­sticas, la matriz de datos tendrÃ­a un tamaÃ±o de $100 \times 3$.

## Principales operaciones con matrices

Las operaciones con matrices nos permiten transformar y combinar datos a gran escala.

**Suma y Resta**: La suma y resta de matrices se realiza elemento a elemento, de manera similar a los vectores. Para que la operaciÃ³n sea posible, ambas matrices deben tener las mismas dimensiones.

$$
\mathbf{A} + \mathbf{B} = \begin{bmatrix} a_{11}+b_{11} & \dots & a_{1n}+b_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1}+b_{m1} & \dots & a_{mn}+b_{mn} \end{bmatrix}
$$

**MultiplicaciÃ³n por un Escalar**: Multiplicar una matriz por un escalar implica multiplicar cada elemento de la matriz por ese escalar. Esta operaciÃ³n es Ãºtil para escalar o normalizar todo un conjunto de datos.

$$
c\mathbf{A} = \begin{bmatrix} ca_{11} & \dots & ca_{1n} \\ \vdots & \ddots & \vdots \\ ca_{m1} & \dots & ca_{mn} \end{bmatrix}
$$

**MultiplicaciÃ³n de Matrices**: La multiplicaciÃ³n de matrices es la operaciÃ³n mÃ¡s compleja y, al mismo tiempo, una de las mÃ¡s cruciales en el **Deep Learning**. El resultado de multiplicar una matriz $\mathbf{A}$ de $m \times n$ por una matriz $\mathbf{B}$ de $n \times p$ es una nueva matriz $\mathbf{C}$ de $m \times p$. **La condiciÃ³n es que el nÃºmero de columnas de la primera matriz ($n$) debe ser igual al nÃºmero de filas de la segunda matriz ($n$).**

Cada elemento $c_{ij}$ de la matriz resultante se calcula como el producto escalar de la fila $i$ de la primera matriz y la columna $j$ de la segunda.

$$
c_{ij} = \sum_{k=1}^n a_{ik}b_{kj}
$$

Esta operaciÃ³n es fundamental en las **redes neuronales**, donde la informaciÃ³n se propaga de una capa a otra a travÃ©s de multiplicaciones matriciales. Los pesos de la red se organizan en matrices, y la multiplicaciÃ³n matricial permite calcular las activaciones de las neuronas de la siguiente capa de manera eficiente.

> **Ejemplo:  multiplicaciÃ³n de una matriz $3\times 3$ por un vector**
>
> Consideremos la matriz
>
> $$
> \mathbf{A} =
> \begin{bmatrix}
> 1 & 2 & 3 \\
> 0 & 1 & 4 \\
> 5 & 6 & 0
> \end{bmatrix}
> $$
>
> y el vector
>
> $$
> \mathbf{v} =
> \begin{bmatrix}
> 1 \\
> 2 \\
> 3
> \end{bmatrix}.
> $$
>
> La multiplicaciÃ³n $\mathbf{A}\mathbf{v}$ consiste en calcular el producto escalar de cada fila de la matriz con el vector $\mathbf{v}$.
>
> * Primera fila: $1\cdot 1 + 2\cdot 2 + 3\cdot 3 = 1 + 4 + 9 = 14$
> * Segunda fila: $0\cdot 1 + 1\cdot 2 + 4\cdot 3 = 0 + 2 + 12 = 14$
> * Tercera fila: $5\cdot 1 + 6\cdot 2 + 0\cdot 3 = 5 + 12 + 0 = 17$
>
> El resultado es el nuevo vector
>
> $$
> \mathbf{A}\mathbf{v} =
> \begin{bmatrix}
> 14 \\
> 14 \\
> 17
> \end{bmatrix}.
> $$
>
> ---
>
> #### InterpretaciÃ³n
>
> La matriz $\mathbf{A}$ actÃºa como una transformaciÃ³n sobre el vector $\mathbf{v}$, combinando linealmente sus componentes. Aunque $\mathbf{v}$ tenÃ­a coordenadas $(1,2,3)$, despuÃ©s de la transformaciÃ³n pasa a tener coordenadas $(14,14,17)$.
>
> Este ejemplo muestra que una matriz puede verse como una â€œmÃ¡quinaâ€ que toma un vector y lo convierte en otro, siguiendo reglas de combinaciÃ³n determinadas por sus filas
>



**Matriz Transpuesta**: Es el resultado de intercambiar las filas por las columnas de una matriz. Si una matriz $\mathbf{A}$ tiene dimensiones $m \times n$, su transpuesta $\mathbf{A}^T$ tendrÃ¡ dimensiones $n \times m$. Esta operaciÃ³n es clave en la optimizaciÃ³n de algoritmos.

> El siguiente ejemplo ilustra el concepto de transposiciÃ³n de una matriz, que es un proceso fundamental en el **Ã¡lgebra lineal** y tiene aplicaciones prÃ¡cticas en la **IA**. La transpuesta de una matriz se obtiene al intercambiar sus filas por sus columnas. Si una matriz original tiene dimensiones $m \times n$, su transpuesta tendrÃ¡ dimensiones $n \times m$.
> 
> Consideremos una matriz $\mathbf{A}$ de 3x3:
> 
> $$
> \mathbf{A} = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
> $$
> 
> Para obtener su matriz transpuesta, denotada como $\mathbf{A}^T$, simplemente convertimos las filas de $\mathbf{A}$ en las columnas de $\mathbf{A}^T$.
> 
> La primera fila de $\mathbf{A}$ (1, 2, 3) se convierte en la primera columna de $\mathbf{A}^T$.
> La segunda fila de $\mathbf{A}$ (4, 5, 6) se convierte en la segunda columna de $\mathbf{A}^T$.
> La tercera fila de $\mathbf{A}$ (7, 8, 9) se convierte en la tercera columna de $\mathbf{A}^T$.
> 
> El resultado es la siguiente matriz transpuesta:
> 
> $$
> \mathbf{A}^T = \begin{bmatrix}
1 & 4 & 7 \\
2 & 5 & 8 \\
3 & 6 & 9
\end{bmatrix}
> $$

---

>**Para reflexionar**
> **Â¿Por quÃ© la multiplicaciÃ³n de matrices es una operaciÃ³n tan importante en el Deep Learning?**
> *Considera cÃ³mo los pesos de una capa neuronal y las entradas de la misma se organizan en matrices y cÃ³mo la multiplicaciÃ³n de estas matrices permite calcular de manera paralela y eficiente las activaciones de todas las neuronas de la siguiente capa. Piensa en la diferencia en la complejidad computacional entre realizar los cÃ¡lculos uno por uno frente a usar una operaciÃ³n matricial Ãºnica.*

Tipos especiales de matrices
---

Existen matrices con propiedades particulares que son de gran importancia en la IA. Conocerlas y comprender su comportamiento permite simplificar cÃ¡lculos y entender mejor los algoritmos que dependen de operaciones matriciales.

### **Matriz de Identidad**

Es una matriz cuadrada (mismo nÃºmero de filas y columnas) con unos en la diagonal principal y ceros en el resto. Es el equivalente matricial del nÃºmero 1, ya que multiplicar cualquier matriz por la matriz de identidad no la altera. Por ejemplo, una matriz identidad de dimensiÃ³n tres serÃ­a:

$$
\mathbf{I} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
$$

### La matriz nula

La **matriz nula** es aquella cuyos elementos son todos iguales a cero. Se denota como $\mathbf{0}$ y actÃºa como el **elemento neutro de la suma** de matrices: cualquier matriz sumada a la matriz nula permanece inalterada.

Ejemplo en dimensiÃ³n $3 \times 3$:

$$
\mathbf{0} = \begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
$$

En IA, esta matriz aparece de manera natural al inicializar parÃ¡metros o al representar la ausencia total de informaciÃ³n en un conjunto de caracterÃ­sticas.

---

### Matrices diagonales

Una **matriz diagonal** es aquella en la que todos los elementos fuera de la diagonal principal son cero. Formalmente:

$$
\mathbf{D} = \begin{bmatrix}
d_{11} & 0 & 0 \\
0 & d_{22} & 0 \\
0 & 0 & d_{33}
\end{bmatrix}
$$

El interÃ©s de las matrices diagonales reside en que simplifican enormemente las operaciones: multiplicar por una matriz diagonal equivale a escalar cada componente de un vector. Este comportamiento es clave en algoritmos de reducciÃ³n de dimensionalidad y en transformaciones lineales simples.

---

### Matrices simÃ©tricas

Una **matriz simÃ©trica** es una matriz cuadrada que coincide con su transpuesta, es decir:

$$
\mathbf{A} = \mathbf{A}^T
$$

Ejemplo:

$$
\mathbf{S} = \begin{bmatrix}
2 & 3 & 1 \\
3 & 5 & 4 \\
1 & 4 & 6
\end{bmatrix}
$$

Las matrices simÃ©tricas tienen propiedades muy importantes:

* Todos sus **autovalores** son reales.
* Se pueden **diagonalizar** mediante una base ortogonal.
* Representan relaciones â€œequilibradasâ€ entre variables, como ocurre en las matrices de covarianzas.

En IA, aparecen de forma recurrente en estadÃ­stica y aprendizaje automÃ¡tico, ya que las matrices de covarianzas y correlaciones son siempre simÃ©tricas, y constituyen la base de tÃ©cnicas como el PCA.

> **Para reflexionarâ€¦**
> **Â¿Por quÃ© crees que resulta tan Ãºtil que las matrices de covarianzas sean simÃ©tricas en algoritmos de aprendizaje? Â¿QuÃ© implicaciones tiene que sus autovalores sean siempre reales?**

---

El concepto de rango en matrices
---

El **rango de una matriz** es una medida de la cantidad de informaciÃ³n independiente que contiene. Dicho de otra manera, nos dice cuÃ¡ntas filas o columnas de la matriz aportan realmente algo nuevo y cuÃ¡ntas son redundantes.

Formalmente, el rango se define como el **nÃºmero mÃ¡ximo de filas o columnas que son linealmente independientes entre sÃ­**. En la prÃ¡ctica, esto significa que si alguna fila o columna se puede expresar como combinaciÃ³n de otras, deja de aportar informaciÃ³n adicional y no cuenta para el rango.

Imaginemos una tabla de datos con varias columnas, cada una representando una caracterÃ­stica. Si una columna es, en realidad, la suma o el doble de otra, entonces no aporta nada nuevo. El rango nos alerta de esta redundancia: si tenemos cinco columnas pero solo tres son realmente independientes, el rango de la matriz serÃ¡ 3.

### InterpretaciÃ³n en el contexto de IA

El rango de una matriz de datos puede verse como una medida de la **informaciÃ³n Ãºtil** que tenemos disponible. **Cuanto mayor es el rango, mÃ¡s diversidad de caracterÃ­sticas independientes existen, y por tanto mÃ¡s rica es la representaciÃ³n de los datos.**

Cuando el rango es menor que el nÃºmero total de columnas, significa que algunos de los rasgos que hemos recogido son **repetitivos o redundantes**. Aunque tengamos mÃ¡s columnas, no estamos ganando informaciÃ³n nueva. En la prÃ¡ctica, esto puede llevar a modelos confusos o ineficaces, porque resulta difÃ­cil distinguir quÃ© variables son realmente las responsables de las diferencias en los datos.

### Ejemplos

#### Ejemplo prÃ¡ctico 1

Consideremos la siguiente matriz de $3 \times 3$:

$$
\mathbf{A} =
\begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
1 & 1 & 1
\end{bmatrix}
$$

A primera vista parece que tiene tres filas â€œdistintasâ€, pero observemos con calma:

* La **segunda fila** es exactamente el doble de la primera.
* Eso significa que la segunda no aporta informaciÃ³n nueva, ya que puede construirse como combinaciÃ³n lineal de otra.

En consecuencia, solo la primera y la tercera fila son independientes.

El rango de $\mathbf{A}$ es, por tanto, **2**.

---

##### InterpretaciÃ³n en tÃ©rminos de datos

Podemos imaginar que esta matriz representa un pequeÃ±o conjunto de datos con tres caracterÃ­sticas:

* La primera y la segunda columna no son realmente independientes, porque la segunda es el doble de la primera en todas las filas.
* Solo dos columnas bastan para describir toda la informaciÃ³n presente en la matriz.

Esto nos indica que los datos en $\mathbf{A}$ tienen **redundancia** y que, aunque formalmente tengamos tres variables, solo dos son Ãºtiles de verdad.

#### Ejemplo prÃ¡ctico 2

Supongamos que en una base de datos de viviendas tenemos las siguientes columnas:

* â€œTamaÃ±o en metros cuadradosâ€
* â€œNÃºmero de habitacionesâ€
* â€œTamaÃ±o en pies cuadradosâ€

Aunque parecen tres caracterÃ­sticas, en realidad solo hay dos distintas: los metros cuadrados y el nÃºmero de habitaciones. La columna de pies cuadrados es solo una versiÃ³n escalada de los metros cuadrados. El rango de esta matriz de datos serÃ­a 2, no 3, porque una de las columnas no aporta informaciÃ³n independiente.

> **Para reflexionarâ€¦**
> **Â¿Por quÃ© crees que es problemÃ¡tico almacenar muchas caracterÃ­sticas que en el fondo dicen lo mismo? Â¿QuÃ© ventajas tendrÃ­a identificar estas redundancias antes de trabajar con los datos?**
> **Si quisieras entrenar un modelo con estos datos, Â¿quÃ© problemas crees que surgirÃ­an al incluir una caracterÃ­stica que es exactamente proporcional a otra? Â¿Por quÃ© serÃ­a mÃ¡s razonable quedarse solo con dos columnas?**

---

## Matrices cuadradas y determinante de una matriz

Una primera condiciÃ³n para avanzar en operaciones mÃ¡s complejas es comprender quÃ© significa que una matriz sea **cuadrada**. Llamamos matriz cuadrada a aquella que tiene el mismo nÃºmero de filas que de columnas. Ejemplos comunes serÃ­an una matriz de $2 \times 2$, de $3 \times 3$ o de $n \times n$ en general. Esta condiciÃ³n es indispensable para ciertas operaciones, como veremos cuando queramos calcular la matriz inversa, ya que el concepto mismo de invertir una matriz exige simetrÃ­a en sus dimensiones. **Si una matriz no es cuadrada, no podemos hablar de inversa.**

El siguiente paso es introducir el **determinante**, un nÃºmero Ãºnico que se asocia a cada matriz cuadrada. Aunque pueda parecer un cÃ¡lculo tÃ©cnico mÃ¡s, en realidad el determinante nos ofrece una informaciÃ³n profunda sobre la estructura de la matriz. De forma intuitiva, podemos pensar en Ã©l como un indicador que nos dice si la matriz estÃ¡ â€œbien formadaâ€ o si, por el contrario, contiene redundancias en sus filas o columnas.

### InterpretaciÃ³n del valor del determinante

En efecto, si el determinante de una matriz es **distinto de cero**, significa que sus columnas (o filas) **son linealmente independientes**. En ese caso, la matriz es invertible, lo cual abre la puerta a resolver sistemas de ecuaciones y a aplicar transformaciones sin perder informaciÃ³n.

Si, por el contrario, el determinante es **cero**, la matriz se llama **singular** y no tiene inversa. Esto nos estÃ¡ diciendo que al menos una de sus columnas (o filas) se puede expresar como combinaciÃ³n lineal de las demÃ¡s. En tÃ©rminos prÃ¡cticos, parte de la informaciÃ³n estÃ¡ repetida y la matriz ha â€œcolapsadoâ€ en alguna direcciÃ³n.

### RelaciÃ³n entre rango y determinante

El **rango** y el **determinante** son dos formas distintas de medir la informaciÃ³n contenida en una matriz, pero estÃ¡n estrechamente relacionados. El rango nos indica cuÃ¡ntas filas o columnas son independientes entre sÃ­, mientras que el determinante nos da una medida global de esa independencia en el caso particular de las matrices cuadradas.

Si una matriz cuadrada tiene rango **mÃ¡ximo**, es decir, igual a su nÃºmero de filas o columnas, entonces su determinante es distinto de cero y la matriz es invertible. Por el contrario, si el rango es menor que el tamaÃ±o de la matriz, significa que alguna fila o columna se puede obtener como combinaciÃ³n lineal de otras, y en ese caso el determinante es cero.

En otras palabras:

* **Determinante distinto de cero** âŸ¶ rango mÃ¡ximo âŸ¶ columnas independientes âŸ¶ matriz invertible.
* **Determinante igual a cero** âŸ¶ rango menor al mÃ¡ximo âŸ¶ dependencia lineal âŸ¶ matriz singular.

Esta relaciÃ³n es muy Ãºtil en la prÃ¡ctica, porque el rango nos ofrece una medida general de la independencia (aplicable a cualquier matriz, cuadrada o no), mientras que el determinante actÃºa como un â€œtest rÃ¡pidoâ€ que confirma si una matriz cuadrada tiene independencia completa o si, por el contrario, estÃ¡ degenerada.

---

> **Para reflexionarâ€¦**
> **Â¿Por quÃ© crees que en el anÃ¡lisis de datos resulta mÃ¡s flexible trabajar con el rango que con el determinante? Â¿QuÃ© ventaja ofrece el rango cuando tratamos matrices rectangulares que no son cuadradas?**

---

### RelaciÃ³n con la IA

En el anÃ¡lisis de datos, este concepto conecta directamente con la **independencia de las caracterÃ­sticas**. Las columnas de una matriz de datos representan las variables o rasgos que describen a cada instancia. Si alguna columna puede deducirse de otras (por ejemplo, si â€œtamaÃ±o en pies cuadradosâ€ es siempre proporcional a â€œtamaÃ±o en metros cuadradosâ€), el determinante de la matriz que las contiene serÃ¡ cero.

Esta situaciÃ³n refleja la conocida **multicolinealidad**, un problema serio en algoritmos de aprendizaje automÃ¡tico, porque confunde al modelo e impide determinar la contribuciÃ³n individual de cada caracterÃ­stica. El resultado suele ser un rendimiento inestable y una interpretaciÃ³n poco fiable.

---

### Ejemplo sencillo

Consideremos la matriz

$$
\mathbf{A} =
\begin{bmatrix}
1 & 2 \\
2 & 4
\end{bmatrix}
$$

El determinante se calcula como

$$
|\mathbf{A}| = (1)(4) - (2)(2) = 0
$$

El resultado nos indica que las dos columnas de $\mathbf{A}$ son dependientes (la segunda es el doble de la primera). No hay, por tanto, dos direcciones realmente independientes en el espacio, y la matriz no puede invertirse.

---

> **Para reflexionarâ€¦**
> **Â¿Por quÃ© crees que es peligroso entrenar un modelo con variables redundantes? Â¿QuÃ© pasarÃ­a si intentamos resolver un sistema de ecuaciones con datos que contienen esta dependencia oculta?**

---

## Matriz Inversa: el equivalente a la divisiÃ³n en el mundo de las matrices

En el Ã¡lgebra lineal, la **matriz inversa** ocupa un lugar equivalente al de la divisiÃ³n en el mundo de los nÃºmeros. AsÃ­ como todo nÃºmero real distinto de cero tiene un inverso multiplicativo â€”por ejemplo, $5 \cdot \tfrac{1}{5} = 1$â€”, tambiÃ©n ciertas matrices poseen una inversa que, al multiplicarse por la matriz original, da como resultado la **matriz identidad**.

### DefiniciÃ³n

Formalmente, una matriz cuadrada $\mathbf{A}$ es **invertible** si existe otra matriz $\mathbf{A}^{-1}$ tal que

$$
\mathbf{A} \cdot \mathbf{A}^{-1} = \mathbf{A}^{-1} \cdot \mathbf{A} = \mathbf{I}.
$$

No todas las matrices tienen inversa. Para que una matriz sea invertible debe cumplir dos condiciones:

1. Ser **cuadrada** (mismo nÃºmero de filas y columnas).
2. Tener un **determinante distinto de cero** (lo que garantiza que sus filas o columnas son linealmente independientes).

Cuando alguna de estas condiciones no se cumple, hablamos de una matriz **singular**, que no tiene inversa.

> **Ejemplo:**
>
> Consideremos la matriz
>
> $$
> \mathbf{A} = 
> \begin{bmatrix}
> 1 & 2 \\
> 3 & 4
> \end{bmatrix}.
> $$
>
> El determinante de $\mathbf{A}$ es
>
> $$
> |\mathbf{A}| = (1)(4) - (2)(3) = -2 \neq 0.
> $$
>
> Por tanto, $\mathbf{A}$ es invertible. Su inversa puede calcularse con la fÃ³rmula para matrices $2 \times 2$:
>
> $$
> \mathbf{A}^{-1} = \frac{1}{|\mathbf{A}|}
> \begin{bmatrix}
> d & -b \\
> -c & a
> \end{bmatrix},
> $$
>
> donde $a, b, c, d$ son los elementos de $\mathbf{A}$. Aplicando la fÃ³rmula:
>
> $$
> \mathbf{A}^{-1} = \frac{1}{-2}
> \begin{bmatrix}
> 4 & -2 \\
> -3 & 1
> \end{bmatrix}
> =\\
> \begin{bmatrix}
> -2 & 1 \\
> 1.5 & -0.5
> \end{bmatrix}
> $$
>
> Multiplicar $\mathbf{A}$ por $\mathbf{A}^{-1}$ devuelve la matriz identidad $\mathbf{I}_2$.
>

### InterpretaciÃ³n en de la matriz inversa en el campo de la Inteligencia Artificial

En tÃ©rminos prÃ¡cticos, la matriz inversa aparece al resolver sistemas de ecuaciones lineales del tipo

$$
\mathbf{A}\mathbf{x} = \mathbf{b},
$$

donde $\mathbf{A}$ es una matriz de coeficientes, $\mathbf{x}$ un vector de incÃ³gnitas y $\mathbf{b}$ un vector de resultados. Si $\mathbf{A}$ es invertible, la soluciÃ³n se obtiene directamente como

$$
\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}.
$$

En IA, aunque rara vez se calcula la inversa de manera explÃ­cita en la prÃ¡ctica (porque es costoso computacionalmente), el concepto es clave para entender cÃ³mo los modelos ajustan parÃ¡metros o encuentran soluciones Ãºnicas. La existencia o no de inversa nos habla de si los datos contienen suficiente informaciÃ³n independiente para llegar a una soluciÃ³n estable.

---

> **Para reflexionarâ€¦**
> **Â¿Por quÃ© crees que en problemas con datos redundantes (donde las caracterÃ­sticas no son independientes) no podemos calcular una soluciÃ³n Ãºnica usando la matriz inversa? Â¿QuÃ© alternativas podrÃ­an usarse en esos casos?**

### CÃ³mo calcular la inversa de una matriz: MÃ©todo formal

Aunque en la prÃ¡ctica utilizamos herramientas computacionales como **NumPy** para obtener la inversa de una matriz de forma automÃ¡tica, es importante conocer los mÃ©todos bÃ¡sicos que permiten calcularla. Esto nos ayuda a entender quÃ© significa realmente la operaciÃ³n y por quÃ© no siempre es posible realizarla.

El procedimiento depende del tamaÃ±o de la matriz:

**1. Para matrices $2 \times 2$**
Existe una fÃ³rmula directa. Si

$$
\mathbf{A} =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix},
$$

su inversa es

$$
\mathbf{A}^{-1} = \frac{1}{ad - bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix},
$$

siempre que $ad - bc \neq 0$.

---

**2. Para matrices de mayor dimensiÃ³n**
El cÃ¡lculo requiere un procedimiento mÃ¡s general, basado en tres ideas:

* **Determinante**: asegura si la matriz es invertible (debe ser distinto de cero).
* **Matriz adjunta o de cofactores**: se construye a partir de los determinantes menores de la matriz original.
* **DivisiÃ³n por el determinante**: la inversa se obtiene como

$$
\mathbf{A}^{-1} = \frac{1}{|\mathbf{A}|} \cdot \text{adj}(\mathbf{A}).
$$

Este mÃ©todo, aunque conceptualmente claro, es laborioso de aplicar manualmente para matrices grandes.

---

**3. MÃ©todos algorÃ­tmicos**
En la prÃ¡ctica, cuando se trabaja con matrices de tamaÃ±o elevado, se utilizan mÃ©todos de cÃ¡lculo basados en **descomposiciones matriciales** (como la eliminaciÃ³n de Gauss-Jordan o la descomposiciÃ³n LU). Estos procedimientos son mÃ¡s eficientes y estables numÃ©ricamente, y son los que implementan las bibliotecas de Ã¡lgebra lineal que se emplean en IA.

#### La matriz adjunta

La **matriz adjunta** es una construcciÃ³n que se utiliza para calcular la inversa de una matriz cuadrada.

Para obtenerla, seguimos un procedimiento basado en lo que denominan **cofactores**:

1. Para cada elemento $a_{ij}$ de la matriz original $\mathbf{A}$, se calcula su **menor complementario**, que es el determinante de la submatriz que resulta de eliminar la fila $i$ y la columna $j$ de $\mathbf{A}$.
2. A este menor se le aplica un signo segÃºn la posiciÃ³n, siguiendo la regla $(-1)^{i+j}$, obteniendo asÃ­ el **cofactor** $C_{ij}$.
3. Con todos los cofactores se forma la **matriz de cofactores**.
4. Finalmente, la **matriz adjunta** $\text{adj}(\mathbf{A})$ es la transpuesta de la matriz de cofactores.

En resumen:

$$
\text{adj}(\mathbf{A}) = \text{Cof}(\mathbf{A})^T
$$

donde $\text{Cof}(\mathbf{A})$ es la matriz de cofactores de $\mathbf{A}$.

Una vez construida la matriz adjunta, la inversa de $\mathbf{A}$ se puede calcular mediante la fÃ³rmula:

$$
\mathbf{A}^{-1} = \frac{1}{|\mathbf{A}|}\,\text{adj}(\mathbf{A})
$$

siempre que $|\mathbf{A}| \neq 0$.

---

Perfecto ğŸ™Œ. Entonces preparo un **ejemplo completo con una matriz $3 \times 3$**, para que los alumnos vean cÃ³mo se aplica el mÃ©todo de la adjunta al cÃ¡lculo de la inversa.

---

#### Ejemplo: cÃ¡lculo de la inversa de una matriz $3 \times 3$ usando la adjunta

Consideremos la matriz

$$
\mathbf{A} =
\begin{bmatrix}
1 & 2 & 3 \\
0 & 1 & 4 \\
5 & 6 & 0
\end{bmatrix}.
$$

**1. CÃ¡lculo del determinante**

Para poder calcular la inversa necesitamos que el determinante sea distinto de cero.

$$
|\mathbf{A}| = 1 \cdot (1 \cdot 0 - 4 \cdot 6) - 2 \cdot (0 \cdot 0 - 4 \cdot 5) + 3 \cdot (0 \cdot 6 - 1 \cdot 5)
$$

$$
= 1 \cdot (-24) - 2 \cdot (-20) + 3 \cdot (-5)
= -24 + 40 - 15 = 1
$$

Como $|\mathbf{A}| = 1 \neq 0$, la matriz es invertible.

---

**2. Matriz de cofactores**

Ahora calculamos cada cofactor $C_{ij} = (-1)^{i+j} |M_{ij}|$, donde $M_{ij}$ es la submatriz que se obtiene eliminando la fila $i$ y la columna $j$.

$$
C_{11} = 
\begin{vmatrix} 1 & 4 \\
6 & 0 
\end{vmatrix}
=\\ (1)(0) - (4)(6) = -24
$$

$$
C_{12} = 
-\begin{vmatrix}
 0 & 4 \\
 5 & 0 
 \end{vmatrix} 
= -[(0)(0) - (4)(5)] = -(-20) = 20
$$

$$
C_{13} = 
\begin{vmatrix}
0 & 1 \\
5 & 6
\end{vmatrix}
= (0)(6) - (1)(5) = -5
$$

$$
C_{21} = 
-\begin{vmatrix}
2 & 3 \\
6 & 0
\end{vmatrix}
= -[(2)(0) - (3)(6)] = -(-18) = 18
$$

$$
C_{22} = 
\begin{vmatrix}
1 & 3 \\
5 & 0
\end{vmatrix}
= (1)(0) - (3)(5) = -15
$$

$$
C_{23} = 
-\begin{vmatrix}
1 & 2 \\
5 & 6
\end{vmatrix}
= -[(1)(6) - (2)(5)] = -[6 - 10] = 4
$$

$$
C_{31} = 
\begin{vmatrix}
2 & 3 \\
1 & 4
\end{vmatrix}
= (2)(4) - (3)(1) = 8 - 3 = 5
$$

$$
C_{32} = 
-\begin{vmatrix}
1 & 3 \\
0 & 4
\end{vmatrix}
= -[(1)(4) - (3)(0)] = -4
$$

$$
C_{33} = 
\begin{vmatrix}
1 & 2 \\
0 & 1
\end{vmatrix}
= (1)(1) - (2)(0) = 1
$$

La **matriz de cofactores** es:

$$
\text{Cof}(\mathbf{A}) =
\begin{bmatrix}
-24 & 20 & -5 \\
18 & -15 & 4 \\
5 & -4 & 1
\end{bmatrix}.
$$

---

**3. Matriz adjunta**

La adjunta se obtiene trasponer la matriz de cofactores:

$$
\text{adj}(\mathbf{A}) =
\begin{bmatrix}
-24 & 18 & 5 \\
20 & -15 & -4 \\
-5 & 4 & 1
\end{bmatrix}.
$$

---

**4. Inversa de la matriz**

Finalmente, aplicamos la fÃ³rmula:

$$
\mathbf{A}^{-1} = \frac{1}{|\mathbf{A}|} \, \text{adj}(\mathbf{A}).
$$

Como $|\mathbf{A}| = 1$, la inversa coincide con la adjunta:

$$
\mathbf{A}^{-1} =
\begin{bmatrix}
-24 & 18 & 5 \\
20 & -15 & -4 \\
-5 & 4 & 1
\end{bmatrix}.
$$

---

> **Para reflexionarâ€¦**
> **Si en este ejemplo el determinante hubiera resultado ser cero, Â¿quÃ© habrÃ­a significado en tÃ©rminos de independencia de las filas y columnas de la matriz? Â¿CÃ³mo se relaciona esto con la imposibilidad de calcular la inversa?**

#### InterpretaciÃ³n prÃ¡ctica

MÃ¡s allÃ¡ de la tÃ©cnica concreta, lo importante es comprender que el cÃ¡lculo de la inversa estÃ¡ Ã­ntimamente ligado a la estructura de la matriz:

* Si el determinante es cero, no podremos obtener una inversa, sin importar el mÃ©todo.
* Si el determinante es distinto de cero, siempre existirÃ¡ una estrategia para calcularla, aunque en la prÃ¡ctica lo haremos con un ordenador.

---

> **Para reflexionarâ€¦**
> **Si en un proyecto de IA tuviÃ©ramos que invertir una matriz de datos de tamaÃ±o muy grande, Â¿crees que serÃ­a prÃ¡ctico hacerlo a mano con el mÃ©todo de cofactores? Â¿QuÃ© ventajas ofrecen entonces las bibliotecas computacionales frente a los mÃ©todos teÃ³ricos?**

---

### MÃ©todos alternativos para el cÃ¡lculo de la matriz inversa

El cÃ¡lculo de la inversa de una matriz puede hacerse de varias maneras. Aunque la definiciÃ³n formal mediante la **matriz adjunta** y el **determinante** es Ãºtil para matrices pequeÃ±as, en la prÃ¡ctica existen otros mÃ©todos mÃ¡s eficientes y aplicables a matrices de mayor dimensiÃ³n. Estos mÃ©todos son importantes porque, en el contexto de la inteligencia artificial, solemos trabajar con datos de gran tamaÃ±o y necesitamos estrategias que sean estables y rÃ¡pidas.

---

#### El mÃ©todo de Gauss-Jordan

El mÃ©todo de **Gauss-Jordan** es un procedimiento algorÃ­tmico que permite calcular la inversa de una matriz aplicando operaciones elementales por filas.

El proceso comienza construyendo una **matriz aumentada** en la que a la derecha de la matriz $\mathbf{A}$ se coloca la matriz identidad $\mathbf{I}$:

$$
[\mathbf{A} \,|\, \mathbf{I}]
$$

El objetivo es aplicar transformaciones por filas que reduzcan la parte izquierda hasta convertirla en la matriz identidad. Cuando esto ocurre, la parte derecha se habrÃ¡ transformado en la matriz inversa $\mathbf{A}^{-1}$:

$$
[\mathbf{I} \,|\, \mathbf{A}^{-1}]
$$

Este mÃ©todo resulta especialmente Ãºtil porque es **mecÃ¡nico y sistemÃ¡tico**: basta con seguir una secuencia de operaciones sin necesidad de calcular determinantes ni cofactores. Por este motivo es un enfoque mucho mÃ¡s prÃ¡ctico para la enseÃ±anza y la programaciÃ³n de algoritmos.

#### Descomposiciones matriciales LU y QR

En problemas reales de IA, cuando las matrices son muy grandes, se emplean tÃ©cnicas de **descomposiciÃ³n** que permiten reorganizar la matriz en factores mÃ¡s simples. Estas factorizaciones facilitan la resoluciÃ³n de sistemas lineales sin necesidad de calcular directamente la inversa.

* **DescomposiciÃ³n LU**: toda matriz invertible puede escribirse como el producto de dos matrices, una triangular inferior ($\mathbf{L}$) y otra triangular superior ($\mathbf{U}$). Resolver un sistema de ecuaciones con esta descomposiciÃ³n resulta mucho mÃ¡s rÃ¡pido que calcular la inversa.

* **DescomposiciÃ³n QR**: la matriz se descompone en una matriz ortogonal $\mathbf{Q}$ y una triangular superior $\mathbf{R}$. Este mÃ©todo es muy Ãºtil en problemas de optimizaciÃ³n y en el cÃ¡lculo de autovalores, que son fundamentales para algoritmos de reducciÃ³n de dimensionalidad.

Estas tÃ©cnicas muestran que la **inversa no se calcula directamente**, sino que se evita en favor de procedimientos mÃ¡s eficientes y menos sensibles a errores numÃ©ricos.

#### Por quÃ© casi nunca se calcula la inversa explÃ­cita en IA

En la prÃ¡ctica, calcular la inversa de una matriz de manera explÃ­cita es algo excepcional. Existen varias razones para ello:

* **Costo computacional elevado**: calcular la inversa de matrices grandes requiere muchos cÃ¡lculos, lo que no es eficiente en entornos donde los datos crecen rÃ¡pidamente.
* **Inestabilidad numÃ©rica**: los ordenadores trabajan con nÃºmeros aproximados, y los pequeÃ±os errores de redondeo pueden amplificarse al invertir una matriz.
* **MÃ©todos alternativos mÃ¡s fiables**: en la mayorÃ­a de los casos, resolver el problema que requiere la inversa puede hacerse directamente con descomposiciones LU, QR o tam biÃ©n SVD, sin necesidad de calcular la inversa de forma explÃ­cita.

Por esto, aunque el concepto de inversa es crucial desde el punto de vista teÃ³rico, los algoritmos prÃ¡cticos en IA suelen **evitar calcularla directamente**.

---

> **Para reflexionarâ€¦**
> **Si las herramientas computacionales modernas nos permiten resolver sistemas lineales sin necesidad de invertir matrices, Â¿por quÃ© sigue siendo importante entender quÃ© es la matriz inversa y cuÃ¡les son sus propiedades teÃ³ricas?**

## Autovalores y autovectores: una nueva forma de ver las matrices

Hemos visto que cuando multiplicamos una matriz por un vector, lo habitual es que el resultado sea un vector distinto: cambia su direcciÃ³n y tambiÃ©n su magnitud. Sin embargo, existen casos especiales en los que un vector **mantiene su direcciÃ³n** al ser transformado, aunque su tamaÃ±o cambie. Estos vectores se llaman **autovectores**, y el factor de escala que los acompaÃ±a se llama **autovalor**.

Formalmente, un vector $\mathbf{v}$ es autovector de una matriz cuadrada $\mathbf{A}$ si existe un escalar $\lambda$ tal que

$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}
$$

donde $\lambda$ es el **autovalor** asociado a $\mathbf{v}$.

---

### InterpretaciÃ³n geomÃ©trica

Podemos imaginar que una matriz representa una transformaciÃ³n en el plano: rotar, estirar o comprimir. La mayorÃ­a de los vectores cambian de direcciÃ³n bajo esa transformaciÃ³n, pero algunos se limitan a cambiar de tamaÃ±o sin perder la orientaciÃ³n.

* Si $\lambda > 1$, el vector se estira.
* Si $0 < \lambda < 1$, el vector se encoge.
* Si $\lambda < 0$, el vector tambiÃ©n cambia de sentido.
* Si $\lambda = 0$, el vector colapsa en el vector nulo.



> **Ejemplo:**
>
> Consideremos la matriz
>
> $$
> \mathbf{A} =
> \begin{bmatrix}
> 2 & 0 \\
> 0 & 3
> \end{bmatrix}
> $$
>
> Si multiplicamos $\mathbf{A}$ por el vector 
>
>$$\mathbf{v}_1 =
>\begin{bmatrix}1 \ 0
>\end{bmatrix}
>$$
>, obtenemos
>
> $$
> \mathbf{A}\mathbf{v}_1 =
> \begin{bmatrix}
> 2 & 0 \\
> 0 & 3
> \end{bmatrix}
> \begin{bmatrix}
> 1 \\
> 0
> \end{bmatrix}
> = \\
> \begin{bmatrix}
> 2 \\
> 0
> \end{bmatrix}
> = 2\mathbf{v}_1
> $$
>
> AquÃ­, $\mathbf{v}_1$ es un autovector y su autovalor es $\lambda = 2$.
>
> De manera anÃ¡loga, para $$\mathbf{v}_2 = \begin{bmatrix}0 \ 1\end{bmatrix}$$:
>
> $$
> \mathbf{A}\mathbf{v}_2 =
> \begin{bmatrix}
> 0 \\
> 3
> \end{bmatrix}
> = 3\mathbf{v}_2
> $$
>
> Por tanto, $\mathbf{v}_2$ es un autovector y $\lambda = 3$ su autovalor.
>

---

### Relevancia en inteligencia artificial

Como en otros elementos del Ã¡lgebra lineal, aunque autovalores y autovectores puedan parecer un concepto matemÃ¡tico abstracto, en realidad aparecen de forma natural en distintos Ã¡mbitos de la inteligencia artificial. No nos interesa en un primer momento saber calcular los mismos, sino comprender su significado y entender cÃ³mo permiten simplificar problemas complejos.

#### AnÃ¡lisis de componentes principales (PCA)

En el **AnÃ¡lisis de Componentes Principales (PCA)**, por ejemplo, los datos se organizan a menudo en un espacio con muchas dimensiones. Una matriz de covarianza nos revela cÃ³mo se relacionan esas dimensiones entre sÃ­, y sus autovectores nos indican hacia quÃ© direcciones se extienden mÃ¡s los datos. Dicho de otro modo, los autovectores marcan las direcciones privilegiadas en las que la nube de puntos muestra mayor variabilidad, mientras que los autovalores nos dicen con quÃ© intensidad se produce esa variaciÃ³n. Esta idea es la que permite reducir la dimensionalidad de un conjunto de datos sin perder la mayor parte de la informaciÃ³n relevante, proyectÃ¡ndolos en menos ejes pero mÃ¡s representativos.

por ejemplo, para describir una casa podrÃ­as registrar el nÃºmero de habitaciones, los metros cuadrados, el precio, la antigÃ¼edad, etc. Si representamos estas variables en un espacio de coordenadas, obtenemos una nube de puntos.

Ahora bien, esa nube de datos no se reparte de forma uniforme en todas las direcciones. HabrÃ¡ ciertas direcciones en las que los datos varÃ­en mucho (por ejemplo, casas mÃ¡s grandes suelen tener mÃ¡s habitaciones), y otras en las que apenas cambien.

AquÃ­ entran en juego los autovectores y autovalores de la **matriz de covarianza**:

- Los **autovectores** seÃ±alan esas direcciones privilegiadas donde los datos â€œse abren mÃ¡sâ€ o presentan mayor variabilidad.
- Los **autovalores** indican cuÃ¡nto se extienden los datos en cada direcciÃ³n.

El PCA aprovecha esta idea: en lugar de trabajar con todas las variables originales, proyecta los datos sobre las direcciones mÃ¡s importantes (autovectores con autovalores mÃ¡s grandes). De este modo, se reduce el nÃºmero de dimensiones, pero conservando casi toda la informaciÃ³n esencial.

#### Covarianza y correlaciÃ³n

En el anÃ¡lisis de **covarianza y correlaciÃ³n**, el papel de los autovalores es igualmente revelador. Un autovalor grande significa que hay una direcciÃ³n en la que los datos se dispersan de forma notable, lo cual refleja un patrÃ³n importante. En cambio, autovalores pequeÃ±os se asocian a direcciones donde apenas hay variaciÃ³n, lo que suele ser indicio de ruido o de redundancia entre variables. En tÃ©rminos intuitivos, los autovalores actÃºan como un filtro que nos ayuda a separar lo esencial de lo accesorio.

#### Redes neuronales

En el caso de las **redes neuronales profundas**, los autovalores aparecen en un contexto diferente pero no menos importante. En el entrenamiento de una red neuronal, utilizamos un algoritmo llamado **descenso de gradiente**. Este algoritmo ajusta poco a poco los parÃ¡metros del modelo para que la predicciÃ³n se acerque cada vez mÃ¡s a la realidad.

Pero la â€œsuperficieâ€ que describe el error no es plana: es una especie de paisaje con valles y montaÃ±as. Los autovalores de la **matriz Hessiana** (que recoge las curvaturas de ese paisaje) nos indican si el terreno es estable o problemÃ¡tico: Si hay autovalores **muy grandes**, significa que en ciertas direcciones el paisaje es demasiado empinado: el algoritmo puede dar pasos demasiado bruscos y volverse inestable. Si hay autovalores **muy pequeÃ±os** (cercanos a cero), significa que el paisaje es casi plano en algunas direcciones: el algoritmo avanza con dificultad y el aprendizaje se hace muy lento.

#### Modelos dinÃ¡micos

Algo similar ocurre en los **modelos dinÃ¡micos**, donde se estudia la evoluciÃ³n de un sistema a lo largo del tiempo. Puede tratarse de cÃ³mo se estabiliza la temperatura en una habitaciÃ³n, de cÃ³mo se propaga una enfermedad o de cÃ³mo evoluciona una poblaciÃ³n. Los autovalores de las matrices que describen estos procesos nos dicen si el sistema tenderÃ¡ a equilibrarse, a crecer de manera descontrolada o a entrar en un ciclo de oscilaciones. En este sentido, los autovalores actÃºan como una especie de termÃ³metro de la estabilidad de un proceso dinÃ¡mico.

Por ejemplo, imagina el caso concreto de un modelo dinÃ¡mico que describe cÃ³mo evoluciona la temperatura en una sala. Los autovalores de la matriz que describe esa dinÃ¡mica nos dan informaciÃ³n clave:

- Si todos los autovalores estÃ¡n entre 0 y 1, el sistema tiende a estabilizarse (por ejemplo, la temperatura se equilibra).
- Si algÃºn autovalor es mayor que 1, el sistema crece sin control (una epidemia que se expande rÃ¡pidamente).
- Si los autovalores son negativos o complejos, el sistema puede mostrar oscilaciones o comportamientos mÃ¡s imprevisibles.

------

>**Para reflexionar**
> Â¿Por quÃ© es tan Ãºtil poder identificar direcciones privilegiadas en un conjunto de datos o en la dinÃ¡mica de un sistema? Â¿CÃ³mo nos ayuda esta visiÃ³n a simplificar problemas que, a primera vista, parecen demasiado complejos
>
> Â¿Por quÃ© es Ãºtil identificar vectores que no cambian de orientaciÃ³n bajo la acciÃ³n de una matriz? Â¿QuÃ© nos dice un autovalor grande sobre la importancia de esa direcciÃ³n en los datos?

### La ecuaciÃ³n caracterÃ­stica

Hasta ahora hemos comprendido de manera intuitiva quÃ© significa que un vector sea un autovector y que un nÃºmero asociado sea su autovalor. El siguiente paso es ver cÃ³mo se calculan en la prÃ¡ctica.

Partimos de la definiciÃ³n: un vector $\mathbf{v}$ es autovector de una matriz $\mathbf{A}$ si cumple

$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v}.
$$

Si pasamos todo al mismo lado, obtenemos

$$
(\mathbf{A} - \lambda \mathbf{I}) \mathbf{v} = \mathbf{0}.
$$

Para que esta ecuaciÃ³n tenga soluciones no triviales (es decir, vectores distintos del vector nulo), la matriz $(\mathbf{A} - \lambda \mathbf{I})$ no debe ser invertible. Esto ocurre precisamente cuando su determinante es igual a cero.

De aquÃ­ surge la **ecuaciÃ³n caracterÃ­stica**:

$$
|\mathbf{A} - \lambda \mathbf{I}| = 0.
$$

Resolver esta ecuaciÃ³n nos da los posibles valores de $\lambda$, que son los **autovalores** de la matriz. Una vez obtenidos, podemos calcular los **autovectores** resolviendo el sistema lineal $(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = \mathbf{0}$ para cada autovalor.

> **Ejemplo sencillo en $2\times 2$**
>
> Consideremos la matriz
>
> $$
> \mathbf{A} =
> \begin{bmatrix}
> 2 & 1 \\
> 1 & 2
> \end{bmatrix}.
> $$
>
> La ecuaciÃ³n caracterÃ­stica es
>
> $$
> \det \begin{bmatrix}
> 2 - \lambda & 1 \\
> 1 & 2 - \lambda
> \end{bmatrix} = 0.
> $$
>
> El determinante se desarrolla como
>
> $$
> (2 - \lambda)(2 - \lambda) - (1)(1) = (2 - \lambda)^2 - 1 = 0.
> $$
>
> Resolviendo:
>
> $$
> (2 - \lambda)^2 = 1 \quad \Rightarrow \quad 2 - \lambda = \pm 1.
> $$
>
> De aquÃ­ obtenemos dos autovalores: $\lambda_1 = 1$ y $\lambda_2 = 3$.
>
> Para $\lambda_1 = 1$, sustituimos en $(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = 0$:
>
> $$
> \begin{bmatrix}
> 1 & 1 \\
> 1 & 1
> \end{bmatrix}
> \begin{bmatrix}
> x \\
> y
> \end{bmatrix}
> = \\
> \begin{bmatrix}
> 0 \\
> 0
> \end{bmatrix}.
> $$
>
> Esto implica que $x = -y$, asÃ­ que un autovector es
>
> $$
> \mathbf{v}_1 = \begin{bmatrix}1 \\ -1\end{bmatrix}.
> $$
>
> Para $\lambda_2 = 3$, hacemos lo mismo:
>
> $$
> \begin{bmatrix}
> -1 & 1 \\
> 1 & -1
> \end{bmatrix}
> \begin{bmatrix}
> x \\
> y
> \end{bmatrix}
> = \\
> \begin{bmatrix}
> 0 \\
> 0
> \end{bmatrix}.
> $$
>
> Ahora se cumple $x = y$, y un autovector correspondiente es
>
> $$
> \mathbf{v}_2 = \begin{bmatrix}1 \\ 1\end{bmatrix}.
> $$
>
> ---
>
> **InterpretaciÃ³n del ejemplo**
>
> La matriz $\mathbf{A}$ transforma los vectores del plano de tal manera que los que estÃ¡n alineados con $(1, -1)$ se encogen (autovalor $1$), mientras que los que estÃ¡n alineados con $(1,1)$ se estiran (autovalor $3$). El resto de vectores se transforman en combinaciones de estas dos direcciones principales.
>
> Este ejemplo muestra cÃ³mo los autovalores y autovectores revelan las **direcciones privilegiadas de una transformaciÃ³n lineal** y el **factor de escala** aplicado en cada una.
>

---

>**Para reflexionar**
> Si tuviÃ©ramos un conjunto de datos que se extiende principalmente a lo largo de la direcciÃ³n $(1,1)$, Â¿quÃ© nos dirÃ­a que esa direcciÃ³n tenga un autovalor grande? Â¿CÃ³mo podrÃ­amos usar esta informaciÃ³n para representar los datos de manera mÃ¡s simple?

---

#### RelaciÃ³n entre autovalores, autovectores y la diagonalizaciÃ³n de una matriz

Una Ãºltima propiedad muy interesante de autovalores y autovectores es que, juntos, permiten **reorganizar una matriz** en una forma mucho mÃ¡s simple de interpretar. Esa reorganizaciÃ³n recibe el nombre de **diagonalizaciÃ³n**.

El punto de partida es que una matriz $\mathbf{A}$ puede escribirse como

$$
\mathbf{A}\mathbf{v} = \lambda \mathbf{v},
$$

donde $\mathbf{v}$ es un autovector y $\lambda$ su autovalor. Si reunimos todos los autovectores de la matriz y los colocamos como columnas en una nueva matriz $\mathbf{P}$, y ademÃ¡s ponemos los autovalores en una matriz diagonal $\mathbf{D}$, ocurre algo sorprendente:

$$
\mathbf{A} = \mathbf{P} \mathbf{D} \mathbf{P}^{-1}.
$$

Esta expresiÃ³n es lo que llamamos **diagonalizaciÃ³n**. La matriz $\mathbf{D}$ es diagonal porque en ella solo aparecen los autovalores en la diagonal principal, y la matriz $\mathbf{P}$ contiene los autovectores que â€œexplicanâ€ esa transformaciÃ³n.

##### InterpretaciÃ³n e importancia en IA

Diagonalizar una matriz significa â€œcambiar de perspectivaâ€ para ver el problema en la base formada por los autovectores. En esa nueva base, la transformaciÃ³n que antes era complicada ahora se reduce a algo muy simple: cada autovector se escala por su autovalor, sin mezclarse con los demÃ¡s.

Lo que antes era una transformaciÃ³n general en el espacio se convierte, tras la diagonalizaciÃ³n, en un conjunto de estiramientos y encogimientos independientes, cada uno en su propia direcciÃ³n.

Aunque no siempre se presenta explÃ­citamente con el nombre de diagonalizaciÃ³n, esta idea estÃ¡ detrÃ¡s de muchos algoritmos:

* En el **PCA**, diagonalizar la matriz de covarianza nos permite encontrar las direcciones independientes de mÃ¡xima variabilidad. La matriz diagonal contiene la varianza explicada en cada componente, lo que facilita elegir cuÃ¡les conservar.
* En el anÃ¡lisis de la matriz **Hessiana** de una funciÃ³n de pÃ©rdida, la diagonalizaciÃ³n permite descomponer la curvatura en direcciones independientes, mostrando por quÃ© unas direcciones del aprendizaje son mÃ¡s â€œdifÃ­cilesâ€ que otras.
* En modelos dinÃ¡micos, la diagonalizaciÃ³n ayuda a predecir el comportamiento de un sistema descomponiÃ©ndolo en evoluciones independientes, cada una gobernada por un autovalor.

> **Ejemplo de diagonalizaciÃ³n de una matriz $2\times 2$**
>
> Consideremos la matriz
>
> $$
> \mathbf{A} =
> \begin{bmatrix}
> 2 & 1 \\
> 1 & 2
> \end{bmatrix}.
> $$
>
> Queremos encontrar si es diagonalizable, y en ese caso obtener la descomposiciÃ³n
>
> $$
> \mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1},
> $$
>
> donde $\mathbf{D}$ es diagonal y $\mathbf{P}$ contiene los autovectores.
>
> **Paso 1: cÃ¡lculo de los autovalores**
>
> Partimos de la ecuaciÃ³n caracterÃ­stica:
>
> $$
> |\mathbf{A} - \lambda \mathbf{I}| = 0.
> $$
>
> Es decir:
>
> $$
> \det \begin{bmatrix}
> 2 - \lambda & 1 \\
> 1 & 2 - \lambda
> \end{bmatrix} = (2 - \lambda)(2 - \lambda) - 1 = (2 - \lambda)^2 - 1.
> $$
>
> De aquÃ­ obtenemos
>
> $$
> (2 - \lambda)^2 = 1 \quad \Rightarrow \quad \lambda_1 = 1, \ \lambda_2 = 3.
> $$
>
> **Paso 2: cÃ¡lculo de los autovectores**
>
> Para $\lambda_1 = 1$:
>
> $$
> (\mathbf{A} - \mathbf{I})\mathbf{v} = 
> \begin{bmatrix}
> 1 & 1 \\
> 1 & 1
> \end{bmatrix}
> \begin{bmatrix}
> x \\
> y
> \end{bmatrix}
> = \begin{bmatrix}0 \\ 0\end{bmatrix}.
> $$
>
> Esto implica $x = -y$, por lo que un autovector es
>
> $$
> \mathbf{v}_1 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}.
> $$
>
> Para $\lambda_2 = 3$:
>
> $$
> (\mathbf{A} - 3\mathbf{I})\mathbf{v} =
> \begin{bmatrix}
> -1 & 1 \\
> 1 & -1
> \end{bmatrix}
> \begin{bmatrix}
> x \\
> y
> \end{bmatrix}
> = \begin{bmatrix}0 \\ 0\end{bmatrix}.
> $$
>
> Esto implica $x = y$, por lo que un autovector es
>
> $$
> \mathbf{v}_2 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}.
> $$
>
> **Paso 3: construcciÃ³n de $\mathbf{P}$ y $\mathbf{D}$**
>
> Colocamos los autovectores como columnas de la matriz $\mathbf{P}$:
>
> $$
> \mathbf{P} =
> \begin{bmatrix}
> 1 & 1 \\
> -1 & 1
> \end{bmatrix}.
> $$
>
> Y construimos la matriz diagonal $\mathbf{D}$ con los autovalores en la diagonal:
>
> $$
> \mathbf{D} =
> \begin{bmatrix}
> 1 & 0 \\
> 0 & 3
> \end{bmatrix}.
> $$
>
> **Paso 4: verificaciÃ³n**
>
> Comprobamos que efectivamente
>
> $$
> \mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1}.
> $$
>
> Es decir, al cambiar la base al sistema definido por los autovectores, la matriz $\mathbf{A}$ se convierte en la matriz diagonal $\mathbf{D}$, mucho mÃ¡s fÃ¡cil de interpretar: la direcciÃ³n $(1,-1)$ se escala por $1$ y la direcciÃ³n $(1,1)$ se escala por $3$.
>
> La diagonalizaciÃ³n revela que la transformaciÃ³n definida por $\mathbf{A}$ es, en el fondo, muy simple: en la base original parece mezclar coordenadas, pero en la base de autovectores solo estira en una direcciÃ³n y encoge en otra. Esto es lo que hace que el anÃ¡lisis de problemas complejos se vuelva mÃ¡s comprensible y manejable.
>

---

>**Para reflexionar...**
> **Â¿QuÃ© significa, desde un punto de vista prÃ¡ctico, que una matriz se pueda â€œresumirâ€ como una diagonal en la base de sus autovectores? Â¿CÃ³mo facilita esto la interpretaciÃ³n de un conjunto de datos o de un modelo en IA?**

